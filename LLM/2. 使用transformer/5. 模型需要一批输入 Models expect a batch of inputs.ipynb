{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f7fe84-bdc9-42eb-9d3c-61ef603d0dc8",
   "metadata": {},
   "source": [
    "# å¤ä¹ ä¸€ä¸‹ä¹‹å‰çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c93aa2-8f59-46c0-8e4f-83a6dc40c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56061bd-c22e-4b68-85d8-c4220d76bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ä»checkpointè‡ªåŠ¨åŠ è½½ åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8782e211-a658-4136-b0e1-d7f67490a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdaf3b4-5fcf-4a9f-a205-7c69c9c04f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m input_ids = torch.tensor(ids)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This line will fail.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:976\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    974\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    986\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:770\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m     input_shape = input_ids.size()\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\HF\\Lib\\site-packages\\transformers\\modeling_utils.py:5110\u001b[39m, in \u001b[36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m   5107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   5109\u001b[39m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[32m   5111\u001b[39m     warn_string = (\n\u001b[32m   5112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5115\u001b[39m     )\n\u001b[32m   5117\u001b[39m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[32m   5118\u001b[39m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23379b51-ae27-46f9-8943-43c9fff43ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "# å¤±è´¥äº†ï¼ï¼ï¼ \n",
    "# HF\n",
    "# é—®é¢˜åœ¨äºæˆ‘ä»¬å‘æ¨¡å‹å‘é€äº†ä¸€ä¸ªåºåˆ—ï¼Œè€Œ ğŸ¤— Transformers æ¨¡å‹é»˜è®¤éœ€è¦å¤šä¸ªå¥å­ã€‚\n",
    "# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°è¯•æ‰§è¡Œ tokenizer åœ¨åº”ç”¨äºä¸€ä¸ªåºåˆ—æ—¶åœ¨å¹•åæ‰€åšçš„æ‰€æœ‰æ“ä½œã€‚\n",
    "# ä½†æ˜¯ï¼Œå¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼Œä½ ä¼šå‘ç° tokenizer ä¸ä»…å°†è¾“å…¥ ID åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œè¿˜åœ¨å…¶é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªç»´åº¦ï¼š\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d783aae9-0dea-414d-913a-296e571e1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# è¿›è¡Œæ­£ç¡®çš„æ“ä½œ\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# å¢åŠ ä¸€ä¸ªç»´åº¦\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca687af-de26-4ebc-a065-9cdbd9ebbbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# è¯•ä¸€ä¸‹ä¸¤ä¸ª\n",
    "batched_ids = torch.tensor([ids, ids])\n",
    "output_2 = model(batched_ids)\n",
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8dbda-3df2-47a6-9208-b937ed473ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011f304-4811-4998-9d82-43b2256eb1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193555d4-9222-4666-a37f-c5baee076f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4019d-6325-4c86-9fcc-8198e810df5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f83d67-fae3-4248-b6ba-409bb08c2e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc31df97-6445-4e0c-8474-c60310b21224",
   "metadata": {},
   "source": [
    "# Padding the inputs  å¡«å……è¾“å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f65b4-1e73-4d5a-8dc0-90701ec05e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# HF\n",
    "# ä»¥ä¸‹åˆ—è¡¨åˆ—è¡¨æ— æ³•è½¬æ¢ä¸º Tensorï¼š\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "# ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¡«å……ä½¿æˆ‘ä»¬çš„å¼ é‡å…·æœ‰çŸ©å½¢å½¢çŠ¶ã€‚\n",
    "# Padding é€šè¿‡å‘å€¼è¾ƒå°‘çš„å¥å­æ·»åŠ ä¸€ä¸ªåä¸º padding token çš„ç‰¹æ®Šå•è¯æ¥ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„å¥å­å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚\n",
    "# ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ 10 ä¸ªå¥å­åŒ…å« 10 ä¸ªå•è¯ï¼Œ1 ä¸ªå¥å­åŒ…å« 20 ä¸ªå•è¯ï¼Œåˆ™å¡«å……å°†ç¡®ä¿æ‰€æœ‰å¥å­éƒ½åŒ…å« 20 ä¸ªå•è¯ã€‚\n",
    "# åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œç”Ÿæˆçš„å¼ é‡å¦‚ä¸‹æ‰€ç¤ºï¼š\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc7306-4f05-4df9-b4f1-1238cd0a3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¡«å……ä»¤ç‰Œ ID å¯ä»¥åœ¨ tokenizer.pad_token_id ä¸­æ‰¾åˆ°ã€‚\n",
    "# è®©æˆ‘ä»¬ä½¿ç”¨å®ƒï¼Œå°†æˆ‘ä»¬çš„ä¸¤ä¸ªå¥å­å•ç‹¬å‘é€åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶ä¸€èµ·æ‰¹å¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5cf9c2-2d1a-4c65-8e94-d845a114002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)\n",
    "# HF\n",
    "# æˆ‘ä»¬çš„æ‰¹é‡é¢„æµ‹ä¸­çš„ logit æœ‰é—®é¢˜ï¼šç¬¬äºŒè¡Œåº”è¯¥ä¸ç¬¬äºŒå¥çš„ logit ç›¸åŒï¼Œä½†æˆ‘ä»¬æœ‰å®Œå…¨ä¸åŒçš„å€¼ï¼\n",
    "# è¿™æ˜¯å› ä¸º Transformer æ¨¡å‹çš„å…³é”®ç‰¹å¾æ˜¯å°† æ¯ä¸ª Token ç½®äº  ä¸Šä¸‹æ–‡   ä¸­çš„æ³¨æ„åŠ›å±‚ã€‚\n",
    "# è¿™äº›å°†       è€ƒè™‘ padding tokenï¼Œ      å› ä¸ºå®ƒä»¬ä¼šå¤„ç† sequence çš„æ‰€æœ‰ tokenã€‚\n",
    "# ä¸ºäº†åœ¨é€šè¿‡æ¨¡å‹ä¼ é€’ä¸åŒé•¿åº¦çš„å•ä¸ªå¥å­æ—¶ï¼Œæˆ–è€…åœ¨ä¼ é€’åº”ç”¨äº†ç›¸åŒå¥å­å’Œå¡«å……çš„æ‰¹å¤„ç†æ—¶è·å¾—ç›¸åŒçš„ç»“æœï¼Œ\n",
    "# æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„åŠ›å±‚å¿½ç•¥å¡«å……æ ‡è®°ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨    ï¼ï¼ï¼ï¼æ³¨æ„åŠ›æ©ç æ¥ï¼ï¼ï¼    å®Œæˆçš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86149038-610d-4355-9c07-60dd6dfbe65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a8bed-5e86-4c27-b5d3-e60c90e8ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a59eb-ef0c-4adc-a395-7d9596281e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a42ba6-12a6-4094-9c91-3edb92ca3fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bab4da-2412-467b-b144-1964e63c6b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b911b713-fd6c-4803-b8f0-e61871dd598f",
   "metadata": {},
   "source": [
    "# Attention masks  æ³¨æ„è’™ç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fcb095f-04b4-4536-9ee1-cfa26f97357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# HF\n",
    "# æ³¨æ„åŠ›æ©ç æ˜¯ä¸è¾“å…¥ ID å¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œå¡«å……æœ‰ 0 å’Œ 1ï¼š1 è¡¨ç¤ºåº”è¯¥å…³æ³¨ç›¸åº”çš„ä»¤ç‰Œï¼Œ0 è¡¨ç¤ºä¸åº”å…³æ³¨ç›¸åº”çš„ä»¤ç‰Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dcc794-6988-47f0-8663-774f76fdcf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# è®©æˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›æ©ç å®Œæˆå‰é¢çš„ç¤ºä¾‹ï¼š\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d735482-4267-49b6-9e1f-ccaa94742329",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef5b22-9a24-4de4-904b-2b0edabd9909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b291405-a2a8-4dba-b25e-1f47dd14730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2dc499-1e89-49ab-9638-fb103480363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd71b5-8509-4bf6-ab9e-da0f65b48fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10f747-4435-4170-9575-4d046453c335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f01b0e-10b4-4ce6-ab25-8fa8851b38a1",
   "metadata": {},
   "source": [
    "# æ›´é•¿çš„åºåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659d11a4-a1f3-43e3-a0db-567607b0efdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1431207749.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mæˆªæ–­ æˆ– æ¢æ¨¡å‹\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "æˆªæ–­ æˆ– æ¢æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843faf9-c8cd-477b-ace3-1bb9b9cceda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
